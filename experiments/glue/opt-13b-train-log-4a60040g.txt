11/11/2022 11:54:21 Loading data/canonical_data/facebook/opt-13b/mnli_train.json as task 0
11/11/2022 11:54:30 ####################
11/11/2022 11:54:30 ####################
11/11/2022 11:54:30 ############# Gradient Accumulation Info #############
11/11/2022 11:54:30 number of step: 73446
11/11/2022 11:54:30 number of grad grad_accumulation step: 1
11/11/2022 11:54:30 adjusted number of step: 73446
11/11/2022 11:54:30 ############# Gradient Accumulation Info #############
11/11/2022 11:58:16 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList()
  (bert): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 5120, padding_idx=1)
      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)
      (layers): ModuleList(
        (0): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (1): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (2): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (3): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (4): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (5): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (6): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (7): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (8): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (9): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (10): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (11): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (12): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (13): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (14): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (15): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (16): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (17): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (18): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (19): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (20): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (21): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (22): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (23): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (24): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (25): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (26): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (27): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (28): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (29): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (30): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (31): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (32): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (33): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (34): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (35): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (36): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (37): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (38): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
        (39): OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)
            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=5120, out_features=20480, bias=True)
          (fc2): Linear(in_features=20480, out_features=5120, bias=True)
          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (scoring_list): ModuleList(
    (0): Sequential(
      (0): Pooler(
        (dense): Linear(in_features=5120, out_features=5120, bias=True)
        (dropout): DropoutWrapper()
      )
      (1): Linear(in_features=5120, out_features=3, bias=True)
    )
  )
)

11/11/2022 11:58:16 Total number of params: 12879697923
11/11/2022 11:58:16 At epoch 0
11/11/2022 11:59:19 Task [ 0] updates[     1] train loss[1.42188] remaining[18 days, 0:09:08]
